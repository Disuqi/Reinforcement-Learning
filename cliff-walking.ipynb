{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make('CliffWalking-v0')\n",
    "\n",
    "states = env.observation_space.n\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "q_table=np.zeros((states,actions))\n",
    "v_table=np.zeros(states)\n",
    "\n",
    "alpha=0.6\n",
    "gamma=0.99\n",
    "\n",
    "episodes=100000\n",
    "eps_steps=episodes*2//3\n",
    "max_eps=1\n",
    "min_eps=0.1\n",
    "eps_a=(max_eps-min_eps)/eps_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy(current_state, step_no, trainingOn):\n",
    "    eps = max(min_eps, max_eps - eps_a*step_no)\n",
    "    if np.random.uniform() < eps and trainingOn:\n",
    "        action= np.random.randint(0, actions)\n",
    "    else:\n",
    "        q_values = q_table[current_state]\n",
    "        action = np.argmax(q_values)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(state, action, reward, new_state):\n",
    "    q_table[state, action] = (1-alpha)*q_table[state, action] + alpha*(reward + gamma*v_table[new_state])\n",
    "    v_table[state] = max(q_table[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(episodes, trainingOn):\n",
    "    culumative_reward = 0\n",
    "    cumulative_steps = 0\n",
    "    avg_rewards_list = []\n",
    "\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        episode_step_no=0\n",
    "        done=False\n",
    "        while not done:\n",
    "            action = eps_greedy(state, cumulative_steps, trainingOn)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            episode_step_no += 1\n",
    "            cumulative_steps += 1\n",
    "            episode_reward += reward\n",
    "\n",
    "            if trainingOn:\n",
    "                update_q_table(state, action, reward, new_state)\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "        culumative_reward += episode_reward\n",
    "        avg_rewards_list.append(culumative_reward/(e+1))\n",
    "    \n",
    "    average_reward = culumative_reward/episodes\n",
    "    print(\"Average reward: \", average_reward)\n",
    "    return avg_rewards_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Qtable():\n",
    "    m, n = q_table.shape\n",
    "\n",
    "    print(\"State\\t Left \\t Down \\t Right \\t Up\")\n",
    "    print()\n",
    "    for i in range(m):\n",
    "        print(\"State\", i)\n",
    "        for j in range(n):\n",
    "            print(\"\\t\", q_table[i, j], end=\" \")\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_list = q_learning(episodes, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
